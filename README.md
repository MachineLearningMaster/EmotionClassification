# Project Background and Description
Human emotions are an important aspect of Human-Computer interaction. It can be inferred from multiple modalities like facial expressions, eye movements, skin temperature, EKG, electroencephalogram (EEG) measurement to name a few. Being able to predict a personâ€™s emotional state correctly can have can have several practical applications for e.g. for a pilot about to go on an important mission, patients in rehabilitation centers, mental state of doctors and healthcare workers in stressful positions, risk  and  safety  management  system  used  by mental-health  practitioners or even personalized gaming experience. Dolyb Labs who are known for producing great sound effects in cinema use g.Nautilus PRO wearable EEG device to study emotional responses of their audiences while they are watching videos. The scientists at Dolby Labs try to better understand what makes the audience engaged, what makes their skin blush, increase their heart-rate or give them goose bumps.
# Project Scope
For this practicum project I intend to work with one or more of the publicly available datasets that have emotions data for multiple modalities. These dataset that I have requested access to as follows:
1)	DEAP (dataset for emotional analysis using EEG, Physiological and Video signals (http://www.eecs.qmul.ac.uk/mmv/datasets/deap
2)	SEED IV dataset (http://bcmi.sjtu.edu.cn/~seed/contacts.html)
3)	Kaggle : EEG Brainwave Dataset: Feeling Emotions. https://www.kaggle.com/birdy654/eeg-brainwave-dataset-feeling-emotions  (MUSE  EEG headband using  sensors TP9, AF7, AF8 and TP10)
# Deliverables
The primary goal of the project is to be able to classify EEG data using deep neural networks into the various emotion states. In additional I will try to combine EEG data with other modalities like eye movements and evaluate the impact on accuracy. We will try to compare these results with other conventional shallow methods for e.g. SVM. 
# Experimental Setup and description of the above datasets:
1) DEAP(Dataset of Emotion Analysis using EEG and Physiological and Video Signals) 
This is a benchmark dataset for emotion analysis using the EEG, physiological and video signals. Thirty-two participants are shown 40 videos each with one-minute duration. The facial expressions and the EEG signals were recorded for each participant. The EEG signals were recorded from 32 different channels. Most of the publicly available EEG datasets have fewer amounts of data per participant. For further details refer to:
http://www.eecs.qmul.ac.uk/mmv/datasets/deap/readme.html
2) 15 subjects were shown video clips ellicity happy/sad/neutral/fear emotions and EEG was recorded over 62 channels (with eye-tracking) for 3 sessions per subject (24 trials per session). Refer to the following link for further details about the dataset:
http://bcmi.sjtu.edu.cn/~seed/seed-iv.html#
3) Kaggle EEG Brainwave Dataset:
The data was collected from two people (1 male, 1 female) for 3 minutes per state - positive, neutral, negative. Muse EEG headband was used to record data from TP9, AF7, AF8 and TP10 EEG placements via dry electrodes. Six minutes of resting neutral data is also recorded, stimuli used to evoke the emotions are movie clips shown to the participants.

