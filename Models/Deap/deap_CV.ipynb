{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deap_CV.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUvzuoPYsfJX",
        "colab_type": "code",
        "outputId": "f032eabf-dbb4-4dfc-d0c6-f82666a8989e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNo3WSWKvbQy",
        "colab_type": "code",
        "outputId": "4fc8d9f7-fb8a-4d80-e93e-ee660ba45644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8hmBevXsotW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#drive.mount('/content/gdrive')\n",
        "# Specify the base directory where images are located.  You need to save your data here.\n",
        "base_dir = '/content/drive/My Drive/Deap'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNvMiGGrcQat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "e3b80da6-25a9-42f1-f825-f0dcdd5fcdfe"
      },
      "source": [
        "pip uninstall -y tensorflow"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Successfully uninstalled tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCuEBbFYlRWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 921
        },
        "outputId": "6264cf6a-70b5-40ae-cb1d-aaefb12e8680"
      },
      "source": [
        "pip install tensorflow==1.15.2"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 92kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.29.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 54.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.10.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 52.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow==1.15.2) (46.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.6.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=0d8f28c265e0430f9cbc5371215e327a32ac2e48036559fe88fe4c9e6df2c12b\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorflow-estimator, tensorboard, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "  Found existing installation: tensorboard 2.2.1\n",
            "    Uninstalling tensorboard-2.2.1:\n",
            "      Successfully uninstalled tensorboard-2.2.1\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gast",
                  "tensorboard",
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6km1lWMF2kAm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "5eb6cf4c-f29e-42c7-89f1-df0da70a549c"
      },
      "source": [
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.4.0\n",
            "Summary: TensorFlow helps the tensors flow\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: opensource@google.com\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: enum34, wheel, protobuf, tensorflow-tensorboard, six, numpy\n",
            "Required-by: fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYGYJjZZsWAm",
        "colab_type": "code",
        "outputId": "7243221f-74b4-47cc-f77b-14be8891a3ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import sys\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior() \n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "final_fuse = \"concat\"\n",
        "\n",
        "conv_1_shape = '4*4*32'\n",
        "pool_1_shape = 'None'\n",
        "\n",
        "conv_2_shape = '4*4*64'\n",
        "pool_2_shape = 'None'\n",
        "\n",
        "conv_3_shape = '4*4*128'\n",
        "pool_3_shape = 'None'\n",
        "\n",
        "conv_4_shape = '1*1*13'\n",
        "pool_4_shape = 'None'\n",
        "\n",
        "window_size = 128\n",
        "n_lstm_layers = 2\n",
        "\n",
        "# lstm full connected parameter\n",
        "n_hidden_state = 32\n",
        "print(\"\\nsize of hidden state\", n_hidden_state)\n",
        "n_fc_out = 1024\n",
        "n_fc_in = 1024\n",
        "\n",
        "dropout_prob = 0.5\n",
        "np.random.seed(32)\n",
        "\n",
        "norm_type = '2D'\n",
        "regularization_method = 'dropout'\n",
        "enable_penalty = True\n",
        "\n",
        "cnn_suffix        =\".mat_win_128_cnn_dataset.pkl\"\n",
        "rnn_suffix        =\".mat_win_128_rnn_dataset.pkl\"\n",
        "label_suffix    =\".mat_win_128_labels.pkl\"\n",
        "\n",
        "#data_file    = sys.argv[1]\n",
        "data_file    = 's01'\n",
        "arousal_or_valence = 'AROUSAL'# sys.argv[2]\n",
        "\n",
        "dataset_dir = \"/content/drive/My Drive/Deap/deap_shuffled_data_AROUSAL/\"\n",
        "###load training set\n",
        "with open(dataset_dir + data_file + cnn_suffix, \"rb\") as fp:\n",
        "    cnn_datasets = pickle.load(fp)\n",
        "with open(dataset_dir + data_file + rnn_suffix, \"rb\") as fp:\n",
        "    rnn_datasets = pickle.load(fp)\n",
        "with open(dataset_dir + data_file + label_suffix, \"rb\") as fp:\n",
        "    labels = pickle.load(fp)\n",
        "    labels = np.transpose(labels)\n",
        "    print(\"loaded shape:\",labels.shape)\n",
        "lables_backup = labels\n",
        "print(\"cnn_dataset shape before reshape:\", np.shape(cnn_datasets))\n",
        "cnn_datasets = cnn_datasets.reshape(len(cnn_datasets), window_size, 9,9, 1)\n",
        "print(\"cnn_dataset shape after reshape:\", np.shape(cnn_datasets))\n",
        "one_hot_labels = np.array(list(pd.get_dummies(labels)))\n",
        "\n",
        "labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
        "\n",
        "# shuffle data\n",
        "index = np.array(range(0, len(labels)))\n",
        "np.random.shuffle(index)\n",
        "\n",
        "cnn_datasets   = cnn_datasets[index]\n",
        "rnn_datasets   = rnn_datasets[index]\n",
        "labels  = labels[index]\n",
        "\n",
        "# train_index = [i for i in range(2160)]\n",
        "# test_index = [i for i in range(2160,2400)]\n",
        "\n",
        "# train_index = np.array(train_index)\n",
        "# test_index = np.array(test_index)\n",
        "\n",
        "# cnn_train_x = cnn_datasets[train_index]\n",
        "# rnn_train_x = rnn_datasets[train_index]\n",
        "# train_y     = labels[train_index]\n",
        "# train_sample = len(train_y)\n",
        "# print(\"train sample:\", train_sample)\n",
        "\n",
        "# cnn_test_x  = cnn_datasets[test_index]\n",
        "# rnn_test_x  = rnn_datasets[test_index]\n",
        "# test_y      = labels[test_index]\n",
        "# test_sample = len(cnn_test_x)\n",
        "# print(\"test sample:\", test_sample)\n",
        "\n",
        "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Load and Split dataset End **********\\n\")\n",
        "print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Define parameters and functions Begin: **********\\n\")\n",
        "\n",
        "# input parameter\n",
        "n_input_ele = 32\n",
        "n_time_step = window_size\n",
        "\n",
        "input_channel_num = 1\n",
        "input_height = 9\n",
        "input_width = 9\n",
        "\n",
        "n_labels = 2\n",
        "# training parameter\n",
        "lambda_loss_amount = 0.5\n",
        "training_epochs = 70\n",
        "\n",
        "batch_size = 200\n",
        "\n",
        "\n",
        "# kernel parameter\n",
        "kernel_height_1st = 4\n",
        "kernel_width_1st = 4\n",
        "\n",
        "kernel_height_2nd = 4\n",
        "kernel_width_2nd = 4\n",
        "\n",
        "kernel_height_3rd = 4\n",
        "kernel_width_3rd = 4\n",
        "\n",
        "kernel_height_4th = 1\n",
        "kernel_width_4th = 1\n",
        "\n",
        "kernel_stride = 1\n",
        "conv_channel_num = 32\n",
        "\n",
        "# algorithm parameter\n",
        "learning_rate = 1e-4\n",
        "\n",
        "def weight_variable(shape):\n",
        "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def bias_variable(shape):\n",
        "    initial = tf.constant(0.1, shape=shape)\n",
        "    return tf.Variable(initial)\n",
        "\n",
        "def conv2d(x, W, kernel_stride):\n",
        "    # API: must strides[0]=strides[4]=1\n",
        "    return tf.nn.conv2d(x, W, strides=[1, kernel_stride, kernel_stride, 1], padding='SAME')\n",
        "\n",
        "def apply_conv2d(x, filter_height, filter_width, in_channels, out_channels, kernel_stride):\n",
        "    weight = weight_variable([filter_height, filter_width, in_channels, out_channels])\n",
        "    bias = bias_variable([out_channels])  # each feature map shares the same weight and bias\n",
        "    print(\"weight shape:\", np.shape(weight))\n",
        "    print(\"x shape:\", np.shape(x))\n",
        "    #tf.layers.batch_normalization()\n",
        "    return tf.nn.elu(tf.layers.batch_normalization(conv2d(x, weight, kernel_stride)))\n",
        "\n",
        "def apply_max_pooling(x, pooling_height, pooling_width, pooling_stride):\n",
        "    # API: must ksize[0]=ksize[4]=1, strides[0]=strides[4]=1\n",
        "    return tf.nn.max_pool(x, ksize=[1, pooling_height, pooling_width, 1],\n",
        "                          strides=[1, pooling_stride, pooling_stride, 1], padding='SAME')\n",
        "\n",
        "def apply_fully_connect(x, x_size, fc_size):\n",
        "    fc_weight = weight_variable([x_size, fc_size])\n",
        "    fc_bias = bias_variable([fc_size])\n",
        "    return tf.nn.elu(tf.add(tf.matmul(x, fc_weight), fc_bias))\n",
        "\n",
        "def apply_readout(x, x_size, readout_size):\n",
        "    readout_weight = weight_variable([x_size, readout_size])\n",
        "    readout_bias = bias_variable([readout_size])\n",
        "    return tf.add(tf.matmul(x, readout_weight), readout_bias)\n",
        "\n",
        "print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Define parameters and functions End **********\")\n",
        "print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Define NN structure Begin: **********\")\n",
        "\n",
        "# input placeholder\n",
        "cnn_in = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name='cnn_in')\n",
        "rnn_in = tf.placeholder(tf.float32, shape=[None, n_time_step, n_input_ele], name='rnn_in')\n",
        "Y = tf.placeholder(tf.float32, shape=[None, n_labels], name='Y')\n",
        "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
        "\n",
        "###########################################################################################\n",
        "# add cnn parallel to network\n",
        "###########################################################################################\n",
        "# first CNN layer\n",
        "with tf.name_scope(\"conv_1\"):\n",
        "    conv_1 = apply_conv2d(cnn_in, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride)\n",
        "    print(\"conv_1 shape:\", conv_1.shape)\n",
        "# second CNN layer\n",
        "with tf.name_scope(\"conv_2\"):\n",
        "    conv_2 = apply_conv2d(conv_1, kernel_height_2nd, kernel_width_2nd, conv_channel_num, conv_channel_num * 2,kernel_stride)\n",
        "    print(\"conv_2 shape:\", conv_2.shape)\n",
        "# third CNN layer\n",
        "with tf.name_scope(\"conv_3\"):\n",
        "    conv_3 = apply_conv2d(conv_2, kernel_height_3rd, kernel_width_3rd, conv_channel_num * 2, conv_channel_num * 4,kernel_stride)\n",
        "    print(\"conv_3 shape:\", conv_3.shape)\n",
        "# depth concatenate\n",
        "with tf.name_scope(\"depth_concatenate\"):\n",
        "    cube = tf.reshape(conv_3,[-1,9,9,conv_channel_num * 4 * window_size])\n",
        "    print(\"cube shape:\", cube.shape)\n",
        "# fourth CNN layer\n",
        "with tf.name_scope(\"conv_4\"):\n",
        "    conv_4 = apply_conv2d(cube, kernel_height_4th, kernel_width_4th, conv_channel_num * 4 * window_size, 13,kernel_stride)\n",
        "    print(\"\\nconv_4 shape:\", conv_4.shape)\n",
        "\n",
        "# flatten (13*9*9) cube into a 1053 vector.\n",
        "shape = conv_4.get_shape().as_list()\n",
        "conv_3_flat = tf.reshape(conv_4, [-1, shape[1] * shape[2] * shape[3]])\n",
        "\n",
        "cnn_out_fuse = conv_3_flat\n",
        "###########################################################################################\n",
        "# add lstm parallel to network\n",
        "###########################################################################################\n",
        "# rnn_in         ==>    [batch_size, n_time_step, n_electrode]\n",
        "shape = rnn_in.get_shape().as_list()\n",
        "# rnn_in_flat     ==>    [batch_size*n_time_step, n_electrode]\n",
        "rnn_in_flat = tf.reshape(rnn_in, [-1, shape[2]])\n",
        "# fc_in     ==>    [batch_size*n_time_step, n_electrode]\n",
        "rnn_fc_in = apply_fully_connect(rnn_in_flat, shape[2], n_fc_in)\n",
        "# lstm_in    ==>    [batch_size, n_time_step, n_fc_in]\n",
        "lstm_in = tf.reshape(rnn_fc_in, [-1, n_time_step, n_fc_in])\n",
        "# define lstm cell\n",
        "cells = []\n",
        "for _ in range(n_lstm_layers):\n",
        "    with tf.name_scope(\"LSTM_\"+str(n_lstm_layers)):\n",
        "        cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_state, forget_bias=1.0, state_is_tuple=True)\n",
        "        cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "        cells.append(cell)\n",
        "lstm_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "# init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "# rnn_in size [batch_size, window_size, fc_size]\n",
        "# output ==> [batch, step, n_hidden_state]\n",
        "output, states = tf.nn.dynamic_rnn(lstm_cell, lstm_in,dtype=tf.float32, time_major=False)\n",
        "\n",
        "# output ==> [step, batch, n_hidden_state]\n",
        "# output = tf.transpose(output, [1, 0, 2])\n",
        "# only need the output of last time step\n",
        "output = tf.unstack(tf.transpose(output, [1, 0, 2]), name='lstm_out')\n",
        "rnn_output = output[-1]\n",
        "###########################################################################################\n",
        "# fully connected\n",
        "###########################################################################################\n",
        "# rnn_output ==> [batch, fc_size]\n",
        "shape_rnn_out = rnn_output.get_shape().as_list()\n",
        "# fc_out ==> [batch_size, n_fc_out]\n",
        "lstm_fc_out = apply_fully_connect(rnn_output, shape_rnn_out[1], n_fc_out)\n",
        "# keep_prob = tf.placeholder(tf.float32)\n",
        "lstm_fc_drop = tf.nn.dropout(lstm_fc_out, keep_prob)\n",
        "###########################################################################################\n",
        "# fuse parallel cnn and lstm\n",
        "###########################################################################################\n",
        "print(\"final fuse method: concat\")\n",
        "fuse_cnn_rnn = tf.concat([cnn_out_fuse, lstm_fc_drop], axis=1)\n",
        "\n",
        "fuse_cnn_rnn_shape = fuse_cnn_rnn.get_shape().as_list()\n",
        "print(\"\\nfuse_cnn_rnn:\", fuse_cnn_rnn_shape)\n",
        "# readout layer\n",
        "y_ = apply_readout(fuse_cnn_rnn, fuse_cnn_rnn_shape[1], n_labels)\n",
        "y_pred = tf.argmax(tf.nn.softmax(y_), 1, name=\"y_pred\")\n",
        "y_posi = tf.nn.softmax(y_, name=\"y_posi\")\n",
        "\n",
        "# l2 regularization\n",
        "l2 = lambda_loss_amount * sum(\n",
        "    tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
        ")\n",
        "\n",
        "if enable_penalty:\n",
        "    # cross entropy cost function\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y) + l2, name='loss')\n",
        "    tf.summary.scalar('cost_with_L2',cost)\n",
        "else:\n",
        "    # cross entropy cost function\n",
        "    cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y), name='loss')\n",
        "    tf.summary.scalar('cost',cost)\n",
        "\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# get correctly predicted object and accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y_), 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "tf.summary.scalar('accuracy',accuracy)\n",
        "\n",
        "print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Define NN structure End **********\")\n",
        "\n",
        "print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Train and Test NN Begin: **********\")\n",
        "\n",
        "config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth = True\n",
        "config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
        "\n",
        "merged = tf.summary.merge_all()\n",
        "logdir = \"my_tensorboard\"\n",
        "train_writer = tf.summary.FileWriter(\"log/\"+logdir+\"/train\")\n",
        "test_writer = tf.summary.FileWriter(\"log/\"+logdir+\"/test\")\n",
        "\n",
        "fold = 10\n",
        "for curr_fold in range(fold):\n",
        "    fold_size = cnn_datasets.shape[0]//fold\n",
        "    indexes_list = [i for i in range(len(cnn_datasets))]\n",
        "    indexes = np.array(indexes_list)\n",
        "    split_list = [i for i in range(curr_fold*fold_size,(curr_fold+1)*fold_size)]\n",
        "    split = np.array(split_list)\n",
        "    cnn_test_x = cnn_datasets[split] \n",
        "    test_y = labels[split]\n",
        "    rnn_test_x = rnn_datasets[split]\n",
        "\n",
        "    split = np.array(list(set(indexes_list)^set(split_list)))\n",
        "    cnn_train_x = cnn_datasets[split]\n",
        "    rnn_train_x = rnn_datasets[split]\n",
        "    train_y = labels[split]\n",
        "    train_sample = train_y.shape[0]\n",
        "    print(\"training examples:\", train_sample)\n",
        "    test_sample = test_y.shape[0]\n",
        "    print(\"test examples    :\",test_sample)\n",
        "    # set train batch number per epoch\n",
        "    batch_num_per_epoch = math.floor(cnn_train_x.shape[0]/batch_size)+ 1\n",
        "\n",
        "    # set test batch number per epoch\n",
        "    accuracy_batch_size = batch_size\n",
        "    train_accuracy_batch_num = batch_num_per_epoch\n",
        "    test_accuracy_batch_num = math.floor(cnn_test_x.shape[0]/batch_size)+ 1\n",
        "\n",
        "    # print label\n",
        "    one_hot_labels = np.array(list(pd.get_dummies(lables_backup)))\n",
        "    print(one_hot_labels)\n",
        "\n",
        "    with tf.Session(config=config) as session:\n",
        "        train_writer.add_graph(session.graph)\n",
        "        count_cost = 0\n",
        "        train_count_accuracy = 0\n",
        "        test_count_accuracy = 0\n",
        "\n",
        "        session.run(tf.global_variables_initializer())\n",
        "        train_accuracy_save = np.zeros(shape=[0], dtype=float)\n",
        "        test_accuracy_save = np.zeros(shape=[0], dtype=float)\n",
        "        test_loss_save = np.zeros(shape=[0], dtype=float)\n",
        "        train_loss_save = np.zeros(shape=[0], dtype=float)\n",
        "        for epoch in range(training_epochs):\n",
        "            print(\"learning rate: \",learning_rate)\n",
        "            cost_history = np.zeros(shape=[0], dtype=float)\n",
        "            for b in range(batch_num_per_epoch):\n",
        "                start = b* batch_size\n",
        "                if (b+1)*batch_size>train_y.shape[0]:\n",
        "                    offset = train_y.shape[0] % batch_size\n",
        "                else:\n",
        "                    offset = batch_size\n",
        "                cnn_batch = cnn_train_x[start:(start + offset), :, :, :, :]\n",
        "                cnn_batch = cnn_batch.reshape(len(cnn_batch) * window_size, 9, 9, 1)\n",
        "                rnn_batch = rnn_train_x[start:(start + offset), :, :]\n",
        "                batch_y = train_y[start:(start + offset), :]\n",
        "                _ , c = session.run([optimizer, cost],\n",
        "                                   feed_dict={cnn_in: cnn_batch, rnn_in: rnn_batch, Y: batch_y, keep_prob: 1 - dropout_prob,\n",
        "                                              phase_train: True})\n",
        "                cost_history = np.append(cost_history, c)\n",
        "                count_cost += 1\n",
        "            if (epoch % 1 == 0):\n",
        "                train_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "                test_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "                test_loss = np.zeros(shape=[0], dtype=float)\n",
        "                train_loss = np.zeros(shape=[0], dtype=float)\n",
        "\n",
        "                for i in range(train_accuracy_batch_num):\n",
        "                    start = i* batch_size\n",
        "                    if (i+1)*batch_size>train_y.shape[0]:\n",
        "                        offset = train_y.shape[0] % batch_size\n",
        "                    else:\n",
        "                        offset = batch_size\n",
        "                    train_cnn_batch = cnn_train_x[start:(start + offset), :, :, :, :]\n",
        "                    train_cnn_batch = train_cnn_batch.reshape(len(train_cnn_batch) * window_size, 9, 9, 1)\n",
        "\n",
        "                    train_rnn_batch = rnn_train_x[start:(start + offset), :, :]\n",
        "                    train_batch_y = train_y[start:(start + offset), :]\n",
        "\n",
        "                    tf_summary,train_a, train_c = session.run([merged,accuracy, cost],\n",
        "                                                   feed_dict={cnn_in: train_cnn_batch, rnn_in: train_rnn_batch,\n",
        "                                                              Y: train_batch_y, keep_prob: 1.0, phase_train: False})\n",
        "                    train_writer.add_summary(tf_summary,train_count_accuracy)\n",
        "                    train_loss = np.append(train_loss, train_c)\n",
        "                    train_accuracy = np.append(train_accuracy, train_a)\n",
        "                    train_count_accuracy += 1\n",
        "                print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \" Training Cost: \",\n",
        "                      np.mean(train_loss), \"Training Accuracy: \", np.mean(train_accuracy))\n",
        "                train_accuracy_save = np.append(train_accuracy_save, np.mean(train_accuracy))\n",
        "                train_loss_save = np.append(train_loss_save, np.mean(train_loss))\n",
        "\n",
        "                if(np.mean(train_accuracy)<0.8):\n",
        "                    learning_rate=1e-4\n",
        "                elif(0.8<np.mean(train_accuracy)<0.85):\n",
        "                    learning_rate=5e-5\n",
        "                elif(0.85<np.mean(train_accuracy)):\n",
        "                    learning_rate=5e-6\n",
        "\n",
        "                for j in range(test_accuracy_batch_num):\n",
        "                    start = j * batch_size\n",
        "                    print(start)\n",
        "                    if (j+1)*batch_size>test_y.shape[0]:\n",
        "                        offset = test_y.shape[0] % batch_size\n",
        "                    else:\n",
        "                        offset = batch_size\n",
        "                    test_cnn_batch = cnn_test_x[start:(start + offset), :, :, :, :]\n",
        "                    test_cnn_batch = test_cnn_batch.reshape(len(test_cnn_batch) * window_size, 9, 9, 1)\n",
        "\n",
        "                    test_rnn_batch = rnn_test_x[start:(start + offset), :, :]\n",
        "                    test_batch_y = test_y[start:(start + offset), :]\n",
        "\n",
        "                    tf_test_summary,test_a, test_c = session.run([merged,accuracy, cost],\n",
        "                                                 feed_dict={cnn_in: test_cnn_batch, rnn_in: test_rnn_batch, Y: test_batch_y,\n",
        "                                                            keep_prob: 1.0, phase_train: False})\n",
        "                    test_writer.add_summary(tf_test_summary,test_count_accuracy)\n",
        "                    test_accuracy = np.append(test_accuracy, test_a)\n",
        "                    test_loss = np.append(test_loss, test_c)\n",
        "                    test_count_accuracy += 1 \n",
        "                print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \" Test Cost: \",\n",
        "                      np.mean(test_loss), \"Test Accuracy: \", np.mean(test_accuracy), \"\\n\")\n",
        "                test_accuracy_save = np.append(test_accuracy_save, np.mean(test_accuracy))\n",
        "                test_loss_save = np.append(test_loss_save, np.mean(test_loss))\n",
        "            # reshuffle\n",
        "            index = np.array(range(0, len(train_y)))\n",
        "            np.random.shuffle(index)\n",
        "            cnn_train_x=cnn_train_x[index]\n",
        "            rnn_train_x=rnn_train_x[index]\n",
        "            train_y=train_y[index]\n",
        "\n",
        "            # learning_rate decay\n",
        "            if(np.mean(train_accuracy)<0.9):\n",
        "                learning_rate=1e-4\n",
        "            elif(0.9<np.mean(train_accuracy)<0.95):\n",
        "                learning_rate=5e-5\n",
        "            elif(0.99<np.mean(train_accuracy)):\n",
        "                learning_rate=5e-6\n",
        "\n",
        "        test_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "        test_loss = np.zeros(shape=[0], dtype=float)\n",
        "        test_pred = np.zeros(shape=[0], dtype=float)\n",
        "        test_true = np.zeros(shape=[0, 2], dtype=float)\n",
        "        test_posi = np.zeros(shape=[0, 2], dtype=float)\n",
        "        for k in range(test_accuracy_batch_num):\n",
        "            start = k * batch_size\n",
        "            if (k+1)*batch_size>test_y.shape[0]:\n",
        "                offset = test_y.shape[0] % batch_size\n",
        "            else:\n",
        "                offset = batch_size\n",
        "            test_cnn_batch = cnn_test_x[start:(start + offset), :, :, :, :]\n",
        "            test_cnn_batch = test_cnn_batch.reshape(len(test_cnn_batch) * window_size, 9, 9, 1)\n",
        "            test_rnn_batch = rnn_test_x[start:(start + offset), :, :]\n",
        "            test_batch_y = test_y[start:(start + offset), :]\n",
        "\n",
        "            test_a, test_c, test_p, test_r = session.run([accuracy, cost, y_pred, y_posi],\n",
        "                                                         feed_dict={cnn_in: test_cnn_batch, rnn_in: test_rnn_batch,\n",
        "                                                                    Y: test_batch_y, keep_prob: 1.0, phase_train: False})\n",
        "            test_t = test_batch_y\n",
        "\n",
        "            test_accuracy = np.append(test_accuracy, test_a)\n",
        "            test_loss = np.append(test_loss, test_c)\n",
        "            test_pred = np.append(test_pred, test_p)\n",
        "            test_true = np.vstack([test_true, test_t])\n",
        "            test_posi = np.vstack([test_posi, test_r])\n",
        "        test_pred_1_hot = np.asarray(pd.get_dummies(test_pred), dtype=np.int8)\n",
        "        test_true_list = tf.argmax(test_true, 1).eval()\n",
        "        # recall\n",
        "        test_recall = recall_score(test_true, test_pred_1_hot, average=None)\n",
        "        # precision\n",
        "        test_precision = precision_score(test_true, test_pred_1_hot, average=None)\n",
        "        # f1 score\n",
        "        test_f1 = f1_score(test_true, test_pred_1_hot, average=None)\n",
        "        # confusion matrix\n",
        "        # confusion_matrix = confusion_matrix(test_true_list, test_pred)\n",
        "        print(\"********************recall:\", test_recall)\n",
        "        print(\"*****************precision:\", test_precision)\n",
        "        print(\"******************f1_score:\", test_f1)\n",
        "        # print(\"**********confusion_matrix:\\n\", confusion_matrix)\n",
        "\n",
        "        print(\"(\" + time.asctime(time.localtime(time.time())) + \") Final Test Cost: \", np.mean(test_loss),\n",
        "              \"Final Test Accuracy: \", np.mean(test_accuracy))\n",
        "\n",
        "        result = pd.DataFrame(\n",
        "            {'epoch': range(1, epoch + 2), \"train_accuracy\": train_accuracy_save, \"test_accuracy\": test_accuracy_save,\n",
        "             \"train_loss\": train_loss_save, \"test_loss\": test_loss_save})\n",
        "\n",
        "        ins = pd.DataFrame({'conv_1': conv_1_shape,'conv_2': conv_2_shape,'conv_3': conv_3_shape, 'conv_4': conv_4_shape,\n",
        "                            'final_fuse': final_fuse,'rnn fc in': n_fc_in, 'rnn fc out': n_fc_out,\n",
        "                            'hidden_size': n_hidden_state, 'accuracy': np.mean(test_accuracy),\n",
        "                            'keep_prob': 1 - dropout_prob,'sliding_window': window_size, \"epoch\": epoch + 1, \"norm\": norm_type,\n",
        "                            \"learning_rate\": learning_rate, \"regularization\": regularization_method,\n",
        "                            \"train_sample\": train_sample, \"test_sample\": test_sample,\"batch_size\":batch_size}, index=[0])\n",
        "        summary = pd.DataFrame({'recall': test_recall, 'precision': test_precision,'f1_score': test_f1})\n",
        "        writer = pd.ExcelWriter(\n",
        "            \"./results/cv_\"+arousal_or_valence+\"/\"+ data_file +\"_\"+str(curr_fold)+\".xlsx\")\n",
        "        ins.to_excel(writer, 'condition', index=False)\n",
        "        result.to_excel(writer, 'result', index=False)\n",
        "        summary.to_excel(writer, 'summary', index=False)\n",
        "        # fpr, tpr, auc\n",
        "        fpr = dict()\n",
        "        tpr = dict()\n",
        "        roc_auc = dict()\n",
        "        i = 0\n",
        "        for key in one_hot_labels:\n",
        "            fpr[key], tpr[key], _ = roc_curve(test_true[:, i], test_posi[:, i])\n",
        "            roc_auc[key] = auc(fpr[key], tpr[key])\n",
        "            roc = pd.DataFrame({\"fpr\": fpr[key], \"tpr\": tpr[key], \"roc_auc\": roc_auc[key]})\n",
        "            roc.to_excel(writer, str(key), index=False)\n",
        "            i += 1\n",
        "        writer.save()\n",
        "        # save model\n",
        "        '''\n",
        "        saver = tf.train.Saver()\n",
        "        saver.save(session,\n",
        "                   \"./result/cnn_rnn_parallel/tune_rnn_layer/\" + output_dir + \"/model_\" + output_file)\n",
        "        '''\n",
        "        print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Train and Test NN End **********\\n\")\n",
        "train_writer.close()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "size of hidden state 32\n",
            "loaded shape: (2400,)\n",
            "cnn_dataset shape before reshape: (2400, 128, 9, 9)\n",
            "cnn_dataset shape after reshape: (2400, 128, 9, 9, 1)\n",
            "**********(Mon Jun  1 01:54:31 2020) Load and Split dataset End **********\n",
            "\n",
            "**********(Mon Jun  1 01:54:31 2020) Define parameters and functions Begin: **********\n",
            "\n",
            "\n",
            "**********(Mon Jun  1 01:54:31 2020) Define parameters and functions End **********\n",
            "\n",
            "**********(Mon Jun  1 01:54:31 2020) Define NN structure Begin: **********\n",
            "weight shape: (4, 4, 1, 32)\n",
            "x shape: (?, 9, 9, 1)\n",
            "WARNING:tensorflow:From <ipython-input-1-3863b9212afe>:160: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "conv_1 shape: (?, 9, 9, 32)\n",
            "weight shape: (4, 4, 32, 64)\n",
            "x shape: (?, 9, 9, 32)\n",
            "conv_2 shape: (?, 9, 9, 64)\n",
            "weight shape: (4, 4, 64, 128)\n",
            "x shape: (?, 9, 9, 64)\n",
            "conv_3 shape: (?, 9, 9, 128)\n",
            "cube shape: (?, 9, 9, 16384)\n",
            "weight shape: (1, 1, 16384, 13)\n",
            "x shape: (?, 9, 9, 16384)\n",
            "\n",
            "conv_4 shape: (?, 9, 9, 13)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-1-3863b9212afe>:231: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-1-3863b9212afe>:234: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-1-3863b9212afe>:238: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-1-3863b9212afe>:253: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "final fuse method: concat\n",
            "\n",
            "fuse_cnn_rnn: [None, 2077]\n",
            "WARNING:tensorflow:From <ipython-input-1-3863b9212afe>:274: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "\n",
            "**********(Mon Jun  1 01:54:33 2020) Define NN structure End **********\n",
            "\n",
            "**********(Mon Jun  1 01:54:33 2020) Train and Test NN Begin: **********\n",
            "training examples: 2160\n",
            "test examples    : 240\n",
            "[0. 1.]\n",
            "learning rate:  0.0001\n",
            "(Mon Jun  1 02:02:37 2020) Epoch:  1  Training Cost:  994.0300792347301 Training Accuracy:  0.7093181826851584\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 02:02:46 2020) Epoch:  1  Test Cost:  995.6441345214844 Test Accuracy:  0.6899999976158142 \n",
            "\n",
            "learning rate:  0.0001\n",
            "(Mon Jun  1 02:10:38 2020) Epoch:  2  Training Cost:  975.0256791548295 Training Accuracy:  0.785227277062156\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 02:10:48 2020) Epoch:  2  Test Cost:  976.4618530273438 Test Accuracy:  0.7049999833106995 \n",
            "\n",
            "learning rate:  0.0001\n",
            "(Mon Jun  1 02:19:20 2020) Epoch:  3  Training Cost:  956.5580888227983 Training Accuracy:  0.8267045454545454\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 02:19:31 2020) Epoch:  3  Test Cost:  957.8364562988281 Test Accuracy:  0.7325000166893005 \n",
            "\n",
            "learning rate:  0.0001\n",
            "(Mon Jun  1 02:29:10 2020) Epoch:  4  Training Cost:  938.3998302112926 Training Accuracy:  0.8594318194822832\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 02:29:22 2020) Epoch:  4  Test Cost:  939.6930541992188 Test Accuracy:  0.7700000107288361 \n",
            "\n",
            "learning rate:  0.0001\n",
            "(Mon Jun  1 02:39:12 2020) Epoch:  5  Training Cost:  920.5615678267045 Training Accuracy:  0.8807954517277804\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 02:39:23 2020) Epoch:  5  Test Cost:  921.8293762207031 Test Accuracy:  0.7775000035762787 \n",
            "\n",
            "learning rate:  0.0001\n",
            "(Mon Jun  1 02:49:41 2020) Epoch:  6  Training Cost:  902.997353293679 Training Accuracy:  0.9004545482722196\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 02:49:53 2020) Epoch:  6  Test Cost:  904.2205200195312 Test Accuracy:  0.7975000143051147 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 02:59:39 2020) Epoch:  7  Training Cost:  885.7357344193892 Training Accuracy:  0.9115909175439314\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 02:59:50 2020) Epoch:  7  Test Cost:  886.9182434082031 Test Accuracy:  0.7975000143051147 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 03:09:10 2020) Epoch:  8  Training Cost:  868.7612471147017 Training Accuracy:  0.9148863716558977\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 03:09:20 2020) Epoch:  8  Test Cost:  869.9377746582031 Test Accuracy:  0.7875000238418579 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 03:18:40 2020) Epoch:  9  Training Cost:  852.0871082652699 Training Accuracy:  0.9163636402650313\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 03:18:52 2020) Epoch:  9  Test Cost:  853.2490234375 Test Accuracy:  0.7950000166893005 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 03:28:16 2020) Epoch:  10  Training Cost:  835.7163529829545 Training Accuracy:  0.9213636409152638\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 03:28:27 2020) Epoch:  10  Test Cost:  836.8404846191406 Test Accuracy:  0.8125 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 03:37:57 2020) Epoch:  11  Training Cost:  819.6469837535511 Training Accuracy:  0.929090906273235\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 03:38:09 2020) Epoch:  11  Test Cost:  820.7457275390625 Test Accuracy:  0.7950000166893005 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 03:47:56 2020) Epoch:  12  Training Cost:  803.8764038085938 Training Accuracy:  0.9340909015048634\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 03:48:07 2020) Epoch:  12  Test Cost:  804.9408874511719 Test Accuracy:  0.8025000095367432 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 03:58:15 2020) Epoch:  13  Training Cost:  788.4029762961648 Training Accuracy:  0.931477275761691\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 03:58:28 2020) Epoch:  13  Test Cost:  789.4385681152344 Test Accuracy:  0.8050000071525574 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 04:08:17 2020) Epoch:  14  Training Cost:  773.2255859375 Training Accuracy:  0.939886369488456\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 04:08:28 2020) Epoch:  14  Test Cost:  774.2130126953125 Test Accuracy:  0.8050000071525574 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 04:17:45 2020) Epoch:  15  Training Cost:  758.3391668146306 Training Accuracy:  0.9432954463091764\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 04:17:56 2020) Epoch:  15  Test Cost:  759.2983703613281 Test Accuracy:  0.800000011920929 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 04:27:39 2020) Epoch:  16  Training Cost:  743.7382479580966 Training Accuracy:  0.9440909082239325\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 04:27:50 2020) Epoch:  16  Test Cost:  744.6734619140625 Test Accuracy:  0.8075000047683716 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 04:36:49 2020) Epoch:  17  Training Cost:  729.4241665926846 Training Accuracy:  0.9470454508608038\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 04:36:59 2020) Epoch:  17  Test Cost:  730.3298950195312 Test Accuracy:  0.8075000047683716 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 04:45:34 2020) Epoch:  18  Training Cost:  715.385331587358 Training Accuracy:  0.9501136324622415\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 04:45:44 2020) Epoch:  18  Test Cost:  716.2594909667969 Test Accuracy:  0.8050000071525574 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 04:54:02 2020) Epoch:  19  Training Cost:  701.6241843483665 Training Accuracy:  0.9487499973990701\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 04:54:12 2020) Epoch:  19  Test Cost:  702.4607543945312 Test Accuracy:  0.8174999952316284 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 05:02:19 2020) Epoch:  20  Training Cost:  688.1324185458096 Training Accuracy:  0.9497727264057506\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 05:02:28 2020) Epoch:  20  Test Cost:  688.9427185058594 Test Accuracy:  0.8174999952316284 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Mon Jun  1 05:10:38 2020) Epoch:  21  Training Cost:  674.9071988192471 Training Accuracy:  0.9557954560626637\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 05:10:48 2020) Epoch:  21  Test Cost:  675.6819458007812 Test Accuracy:  0.8199999928474426 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 05:18:56 2020) Epoch:  22  Training Cost:  661.9450128728694 Training Accuracy:  0.9537499872120944\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 05:19:05 2020) Epoch:  22  Test Cost:  662.6875915527344 Test Accuracy:  0.8199999928474426 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 05:27:24 2020) Epoch:  23  Training Cost:  649.2402232776989 Training Accuracy:  0.9572727192531932\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 05:27:34 2020) Epoch:  23  Test Cost:  649.9479675292969 Test Accuracy:  0.8174999952316284 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 05:35:55 2020) Epoch:  24  Training Cost:  636.7898892489346 Training Accuracy:  0.9560227231545881\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 05:36:05 2020) Epoch:  24  Test Cost:  637.4674072265625 Test Accuracy:  0.8174999952316284 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 05:44:58 2020) Epoch:  25  Training Cost:  624.5856101296165 Training Accuracy:  0.959090915593234\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 05:45:09 2020) Epoch:  25  Test Cost:  625.2380676269531 Test Accuracy:  0.8199999928474426 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 05:53:35 2020) Epoch:  26  Training Cost:  612.6268643465909 Training Accuracy:  0.9618181857195768\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 05:53:45 2020) Epoch:  26  Test Cost:  613.2584838867188 Test Accuracy:  0.8199999928474426 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 06:01:59 2020) Epoch:  27  Training Cost:  600.9076538085938 Training Accuracy:  0.9601136283441023\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 06:02:09 2020) Epoch:  27  Test Cost:  601.5108947753906 Test Accuracy:  0.8299999833106995 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 06:11:20 2020) Epoch:  28  Training Cost:  589.4228626598011 Training Accuracy:  0.9644318277185614\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 06:11:30 2020) Epoch:  28  Test Cost:  589.9900817871094 Test Accuracy:  0.8324999809265137 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 06:19:56 2020) Epoch:  29  Training Cost:  578.1696499911221 Training Accuracy:  0.963181815364144\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 06:20:06 2020) Epoch:  29  Test Cost:  578.7156677246094 Test Accuracy:  0.824999988079071 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 06:28:18 2020) Epoch:  30  Training Cost:  567.1421730735085 Training Accuracy:  0.9647727283564481\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 06:28:28 2020) Epoch:  30  Test Cost:  567.6654968261719 Test Accuracy:  0.8274999856948853 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 06:36:34 2020) Epoch:  31  Training Cost:  556.3369085138494 Training Accuracy:  0.9663636413487521\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 06:36:44 2020) Epoch:  31  Test Cost:  556.8351135253906 Test Accuracy:  0.8299999833106995 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Mon Jun  1 06:44:44 2020) Epoch:  32  Training Cost:  545.7486239346591 Training Accuracy:  0.9689772833477367\n",
            "0\n",
            "200\n",
            "(Mon Jun  1 06:44:53 2020) Epoch:  32  Test Cost:  546.2213745117188 Test Accuracy:  0.8474999964237213 \n",
            "\n",
            "learning rate:  5e-06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}