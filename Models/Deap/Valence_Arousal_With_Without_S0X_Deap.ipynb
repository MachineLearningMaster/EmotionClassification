{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Valence_Arousal_With_Without_S0X_Deap.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "lUvzuoPYsfJX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "f9cb3489-e3e9-4a1d-a0a6-c89ed0048dac"
      },
      "source": [
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qNo3WSWKvbQy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0e081058-a225-48d6-c6fd-f94cc65455b6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pNvMiGGrcQat",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "664364ba-9cd1-4a87-c640-b92ccdf54bb8"
      },
      "source": [
        "# If you are using a tensorflow version higher than 1.15.2 this code wil now work with it. Hence uninstall and install the required version before running the code. You will need to restart the kernel after installating ver 1.15.2\n",
        "pip uninstall -y tensorflow"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling tensorflow-2.2.0:\n",
            "  Successfully uninstalled tensorflow-2.2.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cCuEBbFYlRWm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 844
        },
        "outputId": "a5e98d40-6640-40ca-db37-c135d67aa8e4"
      },
      "source": [
        "pip install tensorflow==1.15.2"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.15.2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9a/d9/fd234c7bf68638423fb8e7f44af7fcfce3bcaf416b51e6d902391e47ec43/tensorflow-1.15.2-cp36-cp36m-manylinux2010_x86_64.whl (110.5MB)\n",
            "\u001b[K     |████████████████████████████████| 110.5MB 43kB/s \n",
            "\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.9.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.8.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.34.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.12.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.0.8)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.29.0)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 62.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.1.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.2.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (3.10.0)\n",
            "Collecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 64.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (1.18.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15.2) (0.2.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.2) (2.10.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (47.3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.2.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (1.6.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.2) (3.1.0)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7540 sha256=8761f514081cd51c2085ce5041934e42de82e04cef32c9db9c34819dc26d5faa\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.10.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "Installing collected packages: gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.3.3\n",
            "    Uninstalling gast-0.3.3:\n",
            "      Successfully uninstalled gast-0.3.3\n",
            "  Found existing installation: tensorboard 2.2.2\n",
            "    Uninstalling tensorboard-2.2.2:\n",
            "      Successfully uninstalled tensorboard-2.2.2\n",
            "  Found existing installation: tensorflow-estimator 2.2.0\n",
            "    Uninstalling tensorflow-estimator-2.2.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0\n",
            "Successfully installed gast-0.2.2 tensorboard-1.15.0 tensorflow-1.15.2 tensorflow-estimator-1.15.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6km1lWMF2kAm",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "84e49d63-c29b-4459-8d72-252afe1e5616"
      },
      "source": [
        "# To determine which version you're using:\n",
        "!pip show tensorflow\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: tensorflow\n",
            "Version: 1.15.2\n",
            "Summary: TensorFlow is an open source machine learning framework for everyone.\n",
            "Home-page: https://www.tensorflow.org/\n",
            "Author: Google Inc.\n",
            "Author-email: packages@tensorflow.org\n",
            "License: Apache 2.0\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: absl-py, grpcio, astor, numpy, opt-einsum, protobuf, termcolor, tensorboard, wheel, six, gast, keras-preprocessing, wrapt, tensorflow-estimator, keras-applications, google-pasta\n",
            "Required-by: fancyimpute\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zo6DULUkax4g",
        "colab_type": "text"
      },
      "source": [
        "#Instructions before running the code: \n",
        "1. Run the pre-processing step as per Deap_Preprocess_v2.ipynb and the pre-processed files are available \n",
        "![image.png](https://drive.google.com/uc?export=view&id=11nE--NRZ_-dQWR_pXFcaZ7TUjXAH31M4)\n",
        "2. Output file directories have been created\n",
        "3. The code runs for one subject, valence or arousal prediction, with and without usage of baseline data. Hence change the following values:\n",
        "arousal_or_valence \n",
        "with_or_without = 'yes' #yes/no\n",
        "data_file_dir    = ['s03']\n",
        "We could as processing of all images in a loop. However I have avoided that as this model is very resource intensive\n",
        "4. Adjust the number of epochs and folds as per the availability of resources at your disposal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bdrKaddBkErT",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FYGYJjZZsWAm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "04ecfcec-15d2-42be-9d56-706d8fbfac13"
      },
      "source": [
        "import sys\n",
        "import sklearn\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import auc\n",
        "import os\n",
        "import pandas as pd\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "#import tensorflow.compat.v1 as tf\n",
        "#tf.disable_v2_behavior() \n",
        "\n",
        "import numpy as np\n",
        "import time\n",
        "import math\n",
        "\n",
        "\n",
        "final_fuse = \"concat\"\n",
        "\n",
        "conv_1_shape = '4*4*32'\n",
        "pool_1_shape = 'None'\n",
        "\n",
        "conv_2_shape = '4*4*64'\n",
        "pool_2_shape = 'None'\n",
        "\n",
        "conv_3_shape = '4*4*128'\n",
        "pool_3_shape = 'None'\n",
        "\n",
        "conv_4_shape = '1*1*13'\n",
        "pool_4_shape = 'None'\n",
        "\n",
        "window_size = 128\n",
        "n_lstm_layers = 2\n",
        "\n",
        "# lstm full connected parameter\n",
        "n_hidden_state = 32\n",
        "print(\"\\nsize of hidden state\", n_hidden_state)\n",
        "n_fc_out = 1024\n",
        "n_fc_in = 1024\n",
        "\n",
        "dropout_prob = 0.5\n",
        "np.random.seed(32)\n",
        "\n",
        "norm_type = '2D'\n",
        "regularization_method = 'dropout'\n",
        "enable_penalty = True\n",
        "\n",
        "cnn_suffix        =\".mat_win_128_cnn_dataset.pkl\"\n",
        "rnn_suffix        =\".mat_win_128_rnn_dataset.pkl\"\n",
        "label_suffix    =\".mat_win_128_labels.pkl\"\n",
        "\n",
        "#data_file    = sys.argv[1]\n",
        "\n",
        "arousal_or_valence = 'arousal'# sys.argv[2]\n",
        "with_or_without = 'yes' #yes/no\n",
        "#data_file_dir    = ['s01','s02','s03','s04','s05','s06','s07','s08','s09','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23']\n",
        "data_file_dir    = ['s03']\n",
        "dataset_dir = \"/content/drive/My Drive/Deap/deap_shuffled_data/\"+with_or_without+\"_\"+arousal_or_valence+\"/\"\n",
        "\n",
        "for data_file in data_file_dir:\n",
        "\n",
        "  ###load training set\n",
        "  with open(dataset_dir + data_file + cnn_suffix, \"rb\") as fp:\n",
        "      cnn_datasets = pickle.load(fp)\n",
        "  with open(dataset_dir + data_file + rnn_suffix, \"rb\") as fp:\n",
        "      rnn_datasets = pickle.load(fp)\n",
        "  with open(dataset_dir + data_file + label_suffix, \"rb\") as fp:\n",
        "      labels = pickle.load(fp)\n",
        "      labels = np.transpose(labels)\n",
        "      print(\"loaded shape:\",labels.shape)\n",
        "  lables_backup = labels\n",
        "  print(\"cnn_dataset shape before reshape:\", np.shape(cnn_datasets))\n",
        "  cnn_datasets = cnn_datasets.reshape(len(cnn_datasets), window_size, 9,9, 1)\n",
        "  print(\"cnn_dataset shape after reshape:\", np.shape(cnn_datasets))\n",
        "  one_hot_labels = np.array(list(pd.get_dummies(labels)))\n",
        "\n",
        "  labels = np.asarray(pd.get_dummies(labels), dtype=np.int8)\n",
        "\n",
        "  # shuffle data\n",
        "  index = np.array(range(0, len(labels)))\n",
        "  np.random.shuffle(index)\n",
        "\n",
        "  cnn_datasets   = cnn_datasets[index]\n",
        "  rnn_datasets   = rnn_datasets[index]\n",
        "  labels  = labels[index]\n",
        "\n",
        "  print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Load and Split dataset End **********\\n\")\n",
        "  print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Define parameters and functions Begin: **********\\n\")\n",
        "\n",
        "  # input parameter\n",
        "  n_input_ele = 32\n",
        "  n_time_step = window_size\n",
        "\n",
        "  input_channel_num = 1\n",
        "  input_height = 9\n",
        "  input_width = 9\n",
        "\n",
        "  n_labels = 2\n",
        "  # training parameter\n",
        "  lambda_loss_amount = 0.5\n",
        "  training_epochs = 10\n",
        "\n",
        "  batch_size = 50 \n",
        "  #200\n",
        "\n",
        "\n",
        "  # kernel parameter\n",
        "  kernel_height_1st = 4\n",
        "  kernel_width_1st = 4\n",
        "\n",
        "  kernel_height_2nd = 4\n",
        "  kernel_width_2nd = 4\n",
        "\n",
        "  kernel_height_3rd = 4\n",
        "  kernel_width_3rd = 4\n",
        "\n",
        "  kernel_height_4th = 1\n",
        "  kernel_width_4th = 1\n",
        "\n",
        "  kernel_stride = 1\n",
        "  conv_channel_num = 32\n",
        "\n",
        "  # algorithm parameter\n",
        "  learning_rate = 1e-4\n",
        "\n",
        "  def weight_variable(shape):\n",
        "      initial = tf.truncated_normal(shape, stddev=0.1)\n",
        "      return tf.Variable(initial)\n",
        "\n",
        "  def bias_variable(shape):\n",
        "      initial = tf.constant(0.1, shape=shape)\n",
        "      return tf.Variable(initial)\n",
        "\n",
        "  def conv2d(x, W, kernel_stride):\n",
        "      # API: must strides[0]=strides[4]=1\n",
        "      return tf.nn.conv2d(x, W, strides=[1, kernel_stride, kernel_stride, 1], padding='SAME')\n",
        "\n",
        "  def apply_conv2d(x, filter_height, filter_width, in_channels, out_channels, kernel_stride):\n",
        "      weight = weight_variable([filter_height, filter_width, in_channels, out_channels])\n",
        "      bias = bias_variable([out_channels])  # each feature map shares the same weight and bias\n",
        "      print(\"weight shape:\", np.shape(weight))\n",
        "      print(\"x shape:\", np.shape(x))\n",
        "      #tf.layers.batch_normalization()\n",
        "      return tf.nn.elu(tf.layers.batch_normalization(conv2d(x, weight, kernel_stride)))\n",
        "\n",
        "  def apply_max_pooling(x, pooling_height, pooling_width, pooling_stride):\n",
        "      # API: must ksize[0]=ksize[4]=1, strides[0]=strides[4]=1\n",
        "      return tf.nn.max_pool(x, ksize=[1, pooling_height, pooling_width, 1],\n",
        "                            strides=[1, pooling_stride, pooling_stride, 1], padding='SAME')\n",
        "\n",
        "  def apply_fully_connect(x, x_size, fc_size):\n",
        "      fc_weight = weight_variable([x_size, fc_size])\n",
        "      fc_bias = bias_variable([fc_size])\n",
        "      return tf.nn.elu(tf.add(tf.matmul(x, fc_weight), fc_bias))\n",
        "\n",
        "  def apply_readout(x, x_size, readout_size):\n",
        "      readout_weight = weight_variable([x_size, readout_size])\n",
        "      readout_bias = bias_variable([readout_size])\n",
        "      return tf.add(tf.matmul(x, readout_weight), readout_bias)\n",
        "\n",
        "  print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Define parameters and functions End **********\")\n",
        "  print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Define NN structure Begin: **********\")\n",
        "\n",
        "  # input placeholder\n",
        "  cnn_in = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name='cnn_in')\n",
        "  rnn_in = tf.placeholder(tf.float32, shape=[None, n_time_step, n_input_ele], name='rnn_in')\n",
        "  Y = tf.placeholder(tf.float32, shape=[None, n_labels], name='Y')\n",
        "  keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "  phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
        "\n",
        "  ###########################################################################################\n",
        "  # add cnn parallel to network\n",
        "  ###########################################################################################\n",
        "  # first CNN layer\n",
        "  with tf.name_scope(\"conv_1\"):\n",
        "      conv_1 = apply_conv2d(cnn_in, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride)\n",
        "      print(\"conv_1 shape:\", conv_1.shape)\n",
        "  # second CNN layer\n",
        "  with tf.name_scope(\"conv_2\"):\n",
        "      conv_2 = apply_conv2d(conv_1, kernel_height_2nd, kernel_width_2nd, conv_channel_num, conv_channel_num * 2,kernel_stride)\n",
        "      print(\"conv_2 shape:\", conv_2.shape)\n",
        "  # third CNN layer\n",
        "  with tf.name_scope(\"conv_3\"):\n",
        "      conv_3 = apply_conv2d(conv_2, kernel_height_3rd, kernel_width_3rd, conv_channel_num * 2, conv_channel_num * 4,kernel_stride)\n",
        "      print(\"conv_3 shape:\", conv_3.shape)\n",
        "  # depth concatenate\n",
        "  with tf.name_scope(\"depth_concatenate\"):\n",
        "      cube = tf.reshape(conv_3,[-1,9,9,conv_channel_num * 4 * window_size])\n",
        "      print(\"cube shape:\", cube.shape)\n",
        "  # fourth CNN layer\n",
        "  with tf.name_scope(\"conv_4\"):\n",
        "      conv_4 = apply_conv2d(cube, kernel_height_4th, kernel_width_4th, conv_channel_num * 4 * window_size, 13,kernel_stride)\n",
        "      print(\"\\nconv_4 shape:\", conv_4.shape)\n",
        "\n",
        "  # flatten (13*9*9) cube into a 1053 vector.\n",
        "  shape = conv_4.get_shape().as_list()\n",
        "  conv_3_flat = tf.reshape(conv_4, [-1, shape[1] * shape[2] * shape[3]])\n",
        "\n",
        "  cnn_out_fuse = conv_3_flat\n",
        "  ###########################################################################################\n",
        "  # add lstm parallel to network\n",
        "  ###########################################################################################\n",
        "  # rnn_in         ==>    [batch_size, n_time_step, n_electrode]\n",
        "  shape = rnn_in.get_shape().as_list()\n",
        "  # rnn_in_flat     ==>    [batch_size*n_time_step, n_electrode]\n",
        "  rnn_in_flat = tf.reshape(rnn_in, [-1, shape[2]])\n",
        "  # fc_in     ==>    [batch_size*n_time_step, n_electrode]\n",
        "  rnn_fc_in = apply_fully_connect(rnn_in_flat, shape[2], n_fc_in)\n",
        "  # lstm_in    ==>    [batch_size, n_time_step, n_fc_in]\n",
        "  lstm_in = tf.reshape(rnn_fc_in, [-1, n_time_step, n_fc_in])\n",
        "  # define lstm cell\n",
        "  cells = []\n",
        "  for _ in range(n_lstm_layers):\n",
        "      with tf.name_scope(\"LSTM_\"+str(n_lstm_layers)):\n",
        "          cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_state, forget_bias=1.0, state_is_tuple=True)\n",
        "          cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "          cells.append(cell)\n",
        "  lstm_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "  output, states = tf.nn.dynamic_rnn(lstm_cell, lstm_in,dtype=tf.float32, time_major=False)\n",
        "\n",
        "  output = tf.unstack(tf.transpose(output, [1, 0, 2]), name='lstm_out')\n",
        "  rnn_output = output[-1]\n",
        "  ###########################################################################################\n",
        "  # fully connected\n",
        "  ###########################################################################################\n",
        "  # rnn_output ==> [batch, fc_size]\n",
        "  shape_rnn_out = rnn_output.get_shape().as_list()\n",
        "  # fc_out ==> [batch_size, n_fc_out]\n",
        "  lstm_fc_out = apply_fully_connect(rnn_output, shape_rnn_out[1], n_fc_out)\n",
        "  # keep_prob = tf.placeholder(tf.float32)\n",
        "  lstm_fc_drop = tf.nn.dropout(lstm_fc_out, keep_prob)\n",
        "  ###########################################################################################\n",
        "  # fuse parallel cnn and lstm\n",
        "  ###########################################################################################\n",
        "  print(\"final fuse method: concat\")\n",
        "  fuse_cnn_rnn = tf.concat([cnn_out_fuse, lstm_fc_drop], axis=1)\n",
        "\n",
        "  fuse_cnn_rnn_shape = fuse_cnn_rnn.get_shape().as_list()\n",
        "  print(\"\\nfuse_cnn_rnn:\", fuse_cnn_rnn_shape)\n",
        "  # readout layer\n",
        "  y_ = apply_readout(fuse_cnn_rnn, fuse_cnn_rnn_shape[1], n_labels)\n",
        "  y_pred = tf.argmax(tf.nn.softmax(y_), 1, name=\"y_pred\")\n",
        "  y_posi = tf.nn.softmax(y_, name=\"y_posi\")\n",
        "\n",
        "  # l2 regularization\n",
        "  l2 = lambda_loss_amount * sum(\n",
        "      tf.nn.l2_loss(tf_var) for tf_var in tf.trainable_variables()\n",
        "  )\n",
        "\n",
        "  if enable_penalty:\n",
        "      # cross entropy cost function\n",
        "      cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y) + l2, name='loss')\n",
        "      tf.summary.scalar('cost_with_L2',cost)\n",
        "  else:\n",
        "      # cross entropy cost function\n",
        "      cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y), name='loss')\n",
        "      tf.summary.scalar('cost',cost)\n",
        "\n",
        "  optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "  # get correctly predicted object and accuracy\n",
        "  correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y_), 1), tf.argmax(Y, 1))\n",
        "  accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "  tf.summary.scalar('accuracy',accuracy)\n",
        "\n",
        "  print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Define NN structure End **********\")\n",
        "\n",
        "  print(\"\\n**********(\" + time.asctime(time.localtime(time.time())) + \") Train and Test NN Begin: **********\")\n",
        "\n",
        "  config = tf.ConfigProto()\n",
        "  # config.gpu_options.allow_growth = True\n",
        "  config.gpu_options.per_process_gpu_memory_fraction = 0.9\n",
        "\n",
        "  merged = tf.summary.merge_all()\n",
        "  logdir = \"my_tensorboard\"\n",
        "  train_writer = tf.summary.FileWriter(\"log/\"+logdir+\"/train\")\n",
        "  test_writer = tf.summary.FileWriter(\"log/\"+logdir+\"/test\")\n",
        "\n",
        "  fold = 10\n",
        "  for curr_fold in range(fold):\n",
        "      fold_size = cnn_datasets.shape[0]//fold\n",
        "      indexes_list = [i for i in range(len(cnn_datasets))]\n",
        "      indexes = np.array(indexes_list)\n",
        "      split_list = [i for i in range(curr_fold*fold_size,(curr_fold+1)*fold_size)]\n",
        "      split = np.array(split_list)\n",
        "      cnn_test_x = cnn_datasets[split] \n",
        "      test_y = labels[split]\n",
        "      rnn_test_x = rnn_datasets[split]\n",
        "\n",
        "      split = np.array(list(set(indexes_list)^set(split_list)))\n",
        "      cnn_train_x = cnn_datasets[split]\n",
        "      rnn_train_x = rnn_datasets[split]\n",
        "      train_y = labels[split]\n",
        "      train_sample = train_y.shape[0]\n",
        "      print(\"training examples:\", train_sample)\n",
        "      test_sample = test_y.shape[0]\n",
        "      print(\"test examples    :\",test_sample)\n",
        "      # set train batch number per epoch\n",
        "      batch_num_per_epoch = math.floor(cnn_train_x.shape[0]/batch_size)+ 1\n",
        "\n",
        "      # set test batch number per epoch\n",
        "      accuracy_batch_size = batch_size\n",
        "      train_accuracy_batch_num = batch_num_per_epoch\n",
        "      test_accuracy_batch_num = math.floor(cnn_test_x.shape[0]/batch_size)+ 1\n",
        "\n",
        "      # print label\n",
        "      one_hot_labels = np.array(list(pd.get_dummies(lables_backup)))\n",
        "      print(one_hot_labels)\n",
        "\n",
        "      with tf.Session(config=config) as session:\n",
        "          train_writer.add_graph(session.graph)\n",
        "          count_cost = 0\n",
        "          train_count_accuracy = 0\n",
        "          test_count_accuracy = 0\n",
        "\n",
        "          session.run(tf.global_variables_initializer())\n",
        "          train_accuracy_save = np.zeros(shape=[0], dtype=float)\n",
        "          test_accuracy_save = np.zeros(shape=[0], dtype=float)\n",
        "          test_loss_save = np.zeros(shape=[0], dtype=float)\n",
        "          train_loss_save = np.zeros(shape=[0], dtype=float)\n",
        "          for epoch in range(training_epochs):\n",
        "              print(\"learning rate: \",learning_rate)\n",
        "              cost_history = np.zeros(shape=[0], dtype=float)\n",
        "              for b in range(batch_num_per_epoch):\n",
        "                  start = b* batch_size\n",
        "                  if (b+1)*batch_size>train_y.shape[0]:\n",
        "                      offset = train_y.shape[0] % batch_size\n",
        "                  else:\n",
        "                      offset = batch_size\n",
        "                  cnn_batch = cnn_train_x[start:(start + offset), :, :, :, :]\n",
        "                  cnn_batch = cnn_batch.reshape(len(cnn_batch) * window_size, 9, 9, 1)\n",
        "                  rnn_batch = rnn_train_x[start:(start + offset), :, :]\n",
        "                  batch_y = train_y[start:(start + offset), :]\n",
        "                  _ , c = session.run([optimizer, cost],\n",
        "                                    feed_dict={cnn_in: cnn_batch, rnn_in: rnn_batch, Y: batch_y, keep_prob: 1 - dropout_prob,\n",
        "                                                phase_train: True})\n",
        "                  cost_history = np.append(cost_history, c)\n",
        "                  count_cost += 1\n",
        "              if (epoch % 1 == 0):\n",
        "                  train_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "                  test_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "                  test_loss = np.zeros(shape=[0], dtype=float)\n",
        "                  train_loss = np.zeros(shape=[0], dtype=float)\n",
        "\n",
        "                  for i in range(train_accuracy_batch_num):\n",
        "                      start = i* batch_size\n",
        "                      if (i+1)*batch_size>train_y.shape[0]:\n",
        "                          offset = train_y.shape[0] % batch_size\n",
        "                      else:\n",
        "                          offset = batch_size\n",
        "                      train_cnn_batch = cnn_train_x[start:(start + offset), :, :, :, :]\n",
        "                      train_cnn_batch = train_cnn_batch.reshape(len(train_cnn_batch) * window_size, 9, 9, 1)\n",
        "\n",
        "                      train_rnn_batch = rnn_train_x[start:(start + offset), :, :]\n",
        "                      train_batch_y = train_y[start:(start + offset), :]\n",
        "\n",
        "                      tf_summary,train_a, train_c = session.run([merged,accuracy, cost],\n",
        "                                                    feed_dict={cnn_in: train_cnn_batch, rnn_in: train_rnn_batch,\n",
        "                                                                Y: train_batch_y, keep_prob: 1.0, phase_train: False})\n",
        "                      train_writer.add_summary(tf_summary,train_count_accuracy)\n",
        "                      train_loss = np.append(train_loss, train_c)\n",
        "                      train_accuracy = np.append(train_accuracy, train_a)\n",
        "                      train_count_accuracy += 1\n",
        "                  print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \" Training Cost: \",\n",
        "                        np.mean(train_loss), \"Training Accuracy: \", np.mean(train_accuracy))\n",
        "                  train_accuracy_save = np.append(train_accuracy_save, np.mean(train_accuracy))\n",
        "                  train_loss_save = np.append(train_loss_save, np.mean(train_loss))\n",
        "\n",
        "                  if(np.mean(train_accuracy)<0.8):\n",
        "                      learning_rate=1e-4\n",
        "                  elif(0.8<np.mean(train_accuracy)<0.85):\n",
        "                      learning_rate=5e-5\n",
        "                  elif(0.85<np.mean(train_accuracy)):\n",
        "                      learning_rate=5e-6\n",
        "\n",
        "                  for j in range(test_accuracy_batch_num):\n",
        "                      start = j * batch_size\n",
        "                      print(start)\n",
        "                      if (j+1)*batch_size>test_y.shape[0]:\n",
        "                          offset = test_y.shape[0] % batch_size\n",
        "                      else:\n",
        "                          offset = batch_size\n",
        "                      test_cnn_batch = cnn_test_x[start:(start + offset), :, :, :, :]\n",
        "                      test_cnn_batch = test_cnn_batch.reshape(len(test_cnn_batch) * window_size, 9, 9, 1)\n",
        "\n",
        "                      test_rnn_batch = rnn_test_x[start:(start + offset), :, :]\n",
        "                      test_batch_y = test_y[start:(start + offset), :]\n",
        "\n",
        "                      tf_test_summary,test_a, test_c = session.run([merged,accuracy, cost],\n",
        "                                                  feed_dict={cnn_in: test_cnn_batch, rnn_in: test_rnn_batch, Y: test_batch_y,\n",
        "                                                              keep_prob: 1.0, phase_train: False})\n",
        "                      test_writer.add_summary(tf_test_summary,test_count_accuracy)\n",
        "                      test_accuracy = np.append(test_accuracy, test_a)\n",
        "                      test_loss = np.append(test_loss, test_c)\n",
        "                      test_count_accuracy += 1 \n",
        "                  print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \" Test Cost: \",\n",
        "                        np.mean(test_loss), \"Test Accuracy: \", np.mean(test_accuracy), \"\\n\")\n",
        "                  test_accuracy_save = np.append(test_accuracy_save, np.mean(test_accuracy))\n",
        "                  test_loss_save = np.append(test_loss_save, np.mean(test_loss))\n",
        "              # reshuffle\n",
        "              index = np.array(range(0, len(train_y)))\n",
        "              np.random.shuffle(index)\n",
        "              cnn_train_x=cnn_train_x[index]\n",
        "              rnn_train_x=rnn_train_x[index]\n",
        "              train_y=train_y[index]\n",
        "\n",
        "              # learning_rate decay\n",
        "              if(np.mean(train_accuracy)<0.9):\n",
        "                  learning_rate=1e-4\n",
        "              elif(0.9<np.mean(train_accuracy)<0.95):\n",
        "                  learning_rate=5e-5\n",
        "              elif(0.99<np.mean(train_accuracy)):\n",
        "                  learning_rate=5e-6\n",
        "\n",
        "          test_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "          test_loss = np.zeros(shape=[0], dtype=float)\n",
        "          test_pred = np.zeros(shape=[0], dtype=float)\n",
        "          test_true = np.zeros(shape=[0, 2], dtype=float)\n",
        "          test_posi = np.zeros(shape=[0, 2], dtype=float)\n",
        "          for k in range(test_accuracy_batch_num):\n",
        "              start = k * batch_size\n",
        "              if (k+1)*batch_size>test_y.shape[0]:\n",
        "                  offset = test_y.shape[0] % batch_size\n",
        "              else:\n",
        "                  offset = batch_size\n",
        "              test_cnn_batch = cnn_test_x[start:(start + offset), :, :, :, :]\n",
        "              test_cnn_batch = test_cnn_batch.reshape(len(test_cnn_batch) * window_size, 9, 9, 1)\n",
        "              test_rnn_batch = rnn_test_x[start:(start + offset), :, :]\n",
        "              test_batch_y = test_y[start:(start + offset), :]\n",
        "\n",
        "              test_a, test_c, test_p, test_r = session.run([accuracy, cost, y_pred, y_posi],\n",
        "                                                          feed_dict={cnn_in: test_cnn_batch, rnn_in: test_rnn_batch,\n",
        "                                                                      Y: test_batch_y, keep_prob: 1.0, phase_train: False})\n",
        "              test_t = test_batch_y\n",
        "\n",
        "              test_accuracy = np.append(test_accuracy, test_a)\n",
        "              test_loss = np.append(test_loss, test_c)\n",
        "              test_pred = np.append(test_pred, test_p)\n",
        "              test_true = np.vstack([test_true, test_t])\n",
        "              test_posi = np.vstack([test_posi, test_r])\n",
        "          test_pred_1_hot = np.asarray(pd.get_dummies(test_pred), dtype=np.int8)\n",
        "          test_true_list = tf.argmax(test_true, 1).eval()\n",
        "          # recall\n",
        "          test_recall = recall_score(test_true, test_pred_1_hot, average=None)\n",
        "          # precision\n",
        "          test_precision = precision_score(test_true, test_pred_1_hot, average=None)\n",
        "          # f1 score\n",
        "          test_f1 = f1_score(test_true, test_pred_1_hot, average=None)\n",
        "          # confusion matrix\n",
        "          # confusion_matrix = confusion_matrix(test_true_list, test_pred)\n",
        "          print(\"********************recall:\", test_recall)\n",
        "          print(\"*****************precision:\", test_precision)\n",
        "          print(\"******************f1_score:\", test_f1)\n",
        "          # print(\"**********confusion_matrix:\\n\", confusion_matrix)\n",
        "\n",
        "          print(\"(\" + time.asctime(time.localtime(time.time())) + \") Final Test Cost: \", np.mean(test_loss),\n",
        "                \"Final Test Accuracy: \", np.mean(test_accuracy))\n",
        "\n",
        "          result = pd.DataFrame(\n",
        "              {'epoch': range(1, epoch + 2), \"train_accuracy\": train_accuracy_save, \"test_accuracy\": test_accuracy_save,\n",
        "              \"train_loss\": train_loss_save, \"test_loss\": test_loss_save})\n",
        "\n",
        "          ins = pd.DataFrame({'conv_1': conv_1_shape,'conv_2': conv_2_shape,'conv_3': conv_3_shape, 'conv_4': conv_4_shape,\n",
        "                              'final_fuse': final_fuse,'rnn fc in': n_fc_in, 'rnn fc out': n_fc_out,\n",
        "                              'hidden_size': n_hidden_state, 'accuracy': np.mean(test_accuracy),\n",
        "                              'keep_prob': 1 - dropout_prob,'sliding_window': window_size, \"epoch\": epoch + 1, \"norm\": norm_type,\n",
        "                              \"learning_rate\": learning_rate, \"regularization\": regularization_method,\n",
        "                              \"train_sample\": train_sample, \"test_sample\": test_sample,\"batch_size\":batch_size}, index=[0])\n",
        "          summary = pd.DataFrame({'recall': test_recall, 'precision': test_precision,'f1_score': test_f1})\n",
        "          \n",
        "          if with_or_without =='no':\n",
        "            file=\"/content/drive/My Drive/Deap/results/origin_\"+arousal_or_valence+\"/\"+ data_file +\"_\"+str(curr_fold)+\".xlsx\"\n",
        "          elif with_or_without =='yes':\n",
        "            file=\"/content/drive/My Drive/Deap/results/cv_\"+arousal_or_valence+\"/\"+ data_file +\"_\"+str(curr_fold)+\".xlsx\"\n",
        "\n",
        "          writer = pd.ExcelWriter(\n",
        "              \"/content/drive/My Drive/Deap/results/cv_\"+arousal_or_valence+\"/\"+ data_file +\"_\"+str(curr_fold)+\".xlsx\")\n",
        "          ins.to_excel(writer, 'condition', index=False)\n",
        "          result.to_excel(writer, 'result', index=False)\n",
        "          summary.to_excel(writer, 'summary', index=False)\n",
        "          # fpr, tpr, auc\n",
        "          fpr = dict()\n",
        "          tpr = dict()\n",
        "          roc_auc = dict()\n",
        "          i = 0\n",
        "          for key in one_hot_labels:\n",
        "              fpr[key], tpr[key], _ = roc_curve(test_true[:, i], test_posi[:, i])\n",
        "              roc_auc[key] = auc(fpr[key], tpr[key])\n",
        "              roc = pd.DataFrame({\"fpr\": fpr[key], \"tpr\": tpr[key], \"roc_auc\": roc_auc[key]})\n",
        "              roc.to_excel(writer, str(key), index=False)\n",
        "              i += 1\n",
        "          writer.save()\n",
        "          # save model\n",
        "          '''\n",
        "          saver = tf.train.Saver()\n",
        "          saver.save(session,\n",
        "                    \"/content/drive/My Drive/Deap/result/cnn_rnn_parallel/tune_rnn_layer/\" + output_dir + \"/model_\" + output_file)\n",
        "          '''\n",
        "          print(\"**********(\" + time.asctime(time.localtime(time.time())) + \") Train and Test NN End **********\\n\")\n",
        "  train_writer.close()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "size of hidden state 32\n",
            "loaded shape: (2400,)\n",
            "cnn_dataset shape before reshape: (2400, 128, 9, 9)\n",
            "cnn_dataset shape after reshape: (2400, 128, 9, 9, 1)\n",
            "**********(Thu Jun 25 19:59:56 2020) Load and Split dataset End **********\n",
            "\n",
            "**********(Thu Jun 25 19:59:56 2020) Define parameters and functions Begin: **********\n",
            "\n",
            "\n",
            "**********(Thu Jun 25 19:59:56 2020) Define parameters and functions End **********\n",
            "\n",
            "**********(Thu Jun 25 19:59:56 2020) Define NN structure Begin: **********\n",
            "weight shape: (4, 4, 1, 32)\n",
            "x shape: (?, 9, 9, 1)\n",
            "WARNING:tensorflow:From <ipython-input-4-7ff64be2dc4b>:166: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/layers/normalization.py:327: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "conv_1 shape: (?, 9, 9, 32)\n",
            "weight shape: (4, 4, 32, 64)\n",
            "x shape: (?, 9, 9, 32)\n",
            "conv_2 shape: (?, 9, 9, 64)\n",
            "weight shape: (4, 4, 64, 128)\n",
            "x shape: (?, 9, 9, 64)\n",
            "conv_3 shape: (?, 9, 9, 128)\n",
            "cube shape: (?, 9, 9, 16384)\n",
            "weight shape: (1, 1, 16384, 13)\n",
            "x shape: (?, 9, 9, 16384)\n",
            "\n",
            "conv_4 shape: (?, 9, 9, 13)\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-4-7ff64be2dc4b>:237: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-4-7ff64be2dc4b>:240: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-4-7ff64be2dc4b>:244: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:735: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:739: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From <ipython-input-4-7ff64be2dc4b>:259: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "final fuse method: concat\n",
            "\n",
            "fuse_cnn_rnn: [None, 2077]\n",
            "WARNING:tensorflow:From <ipython-input-4-7ff64be2dc4b>:280: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "\n",
            "**********(Thu Jun 25 19:59:58 2020) Define NN structure End **********\n",
            "\n",
            "**********(Thu Jun 25 19:59:58 2020) Train and Test NN Begin: **********\n",
            "training examples: 2160\n",
            "test examples    : 240\n",
            "[0. 1.]\n",
            "learning rate:  0.0001\n",
            "(Thu Jun 25 20:00:57 2020) Epoch:  1  Training Cost:  955.7522596879439 Training Accuracy:  0.8904545415531505\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:00:58 2020) Epoch:  1  Test Cost:  956.0781860351562 Test Accuracy:  0.8769999980926514 \n",
            "\n",
            "learning rate:  0.0001\n",
            "(Thu Jun 25 20:01:55 2020) Epoch:  2  Training Cost:  895.3967909379439 Training Accuracy:  0.9254545406861738\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:01:56 2020) Epoch:  2  Test Cost:  895.5618408203125 Test Accuracy:  0.8959999918937683 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Thu Jun 25 20:02:53 2020) Epoch:  3  Training Cost:  836.8772222345525 Training Accuracy:  0.9340909069234674\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:02:54 2020) Epoch:  3  Test Cost:  837.0524291992188 Test Accuracy:  0.8930000066757202 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Thu Jun 25 20:03:52 2020) Epoch:  4  Training Cost:  781.0674091685902 Training Accuracy:  0.9499999948523261\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:03:53 2020) Epoch:  4  Test Cost:  781.2077758789062 Test Accuracy:  0.9119999885559082 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Thu Jun 25 20:04:52 2020) Epoch:  5  Training Cost:  728.34633844549 Training Accuracy:  0.9549999955025587\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:04:53 2020) Epoch:  5  Test Cost:  728.5184692382812 Test Accuracy:  0.902999997138977 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:05:51 2020) Epoch:  6  Training Cost:  678.7074432373047 Training Accuracy:  0.962727275761691\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:05:52 2020) Epoch:  6  Test Cost:  678.837548828125 Test Accuracy:  0.9199999928474426 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:06:50 2020) Epoch:  7  Training Cost:  632.1622258966619 Training Accuracy:  0.9627272703430869\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:06:51 2020) Epoch:  7  Test Cost:  632.2796752929687 Test Accuracy:  0.9239999890327454 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:07:47 2020) Epoch:  8  Training Cost:  588.5700184215199 Training Accuracy:  0.9690909101204439\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:07:48 2020) Epoch:  8  Test Cost:  588.6851196289062 Test Accuracy:  0.9239999771118164 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:08:44 2020) Epoch:  9  Training Cost:  547.8487340753728 Training Accuracy:  0.9690909074111418\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:08:45 2020) Epoch:  9  Test Cost:  547.9381103515625 Test Accuracy:  0.9359999895095825 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:09:42 2020) Epoch:  10  Training Cost:  509.8425195867365 Training Accuracy:  0.972727274352854\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:09:43 2020) Epoch:  10  Test Cost:  509.9183776855469 Test Accuracy:  0.9370000004768372 \n",
            "\n",
            "********************recall: [0.96808511 0.82692308]\n",
            "*****************precision: [0.95287958 0.87755102]\n",
            "******************f1_score: [0.96042216 0.85148515]\n",
            "(Thu Jun 25 20:09:45 2020) Final Test Cost:  509.9183776855469 Final Test Accuracy:  0.9370000004768372\n",
            "**********(Thu Jun 25 20:09:45 2020) Train and Test NN End **********\n",
            "\n",
            "training examples: 2160\n",
            "test examples    : 240\n",
            "[0. 1.]\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:10:43 2020) Epoch:  1  Training Cost:  953.848466352983 Training Accuracy:  0.8768181746656244\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:10:45 2020) Epoch:  1  Test Cost:  953.7843383789062 Test Accuracy:  0.8460000038146973 \n",
            "\n",
            "learning rate:  0.0001\n",
            "(Thu Jun 25 20:11:43 2020) Epoch:  2  Training Cost:  892.5127674449574 Training Accuracy:  0.919090906327421\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:11:44 2020) Epoch:  2  Test Cost:  892.4796752929688 Test Accuracy:  0.903000009059906 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Thu Jun 25 20:12:41 2020) Epoch:  3  Training Cost:  833.4706531871449 Training Accuracy:  0.9286363598975268\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:12:42 2020) Epoch:  3  Test Cost:  833.5051147460938 Test Accuracy:  0.9339999914169311 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Thu Jun 25 20:13:38 2020) Epoch:  4  Training Cost:  777.3680849942294 Training Accuracy:  0.9390909062190489\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:13:39 2020) Epoch:  4  Test Cost:  777.4149047851563 Test Accuracy:  0.9230000019073487 \n",
            "\n",
            "learning rate:  5e-05\n",
            "(Thu Jun 25 20:14:36 2020) Epoch:  5  Training Cost:  724.4715576171875 Training Accuracy:  0.9531818154183301\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:14:37 2020) Epoch:  5  Test Cost:  724.5972900390625 Test Accuracy:  0.9149999976158142 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:15:35 2020) Epoch:  6  Training Cost:  674.7894051291726 Training Accuracy:  0.9645454517819665\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:15:36 2020) Epoch:  6  Test Cost:  674.8860961914063 Test Accuracy:  0.9319999814033508 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:16:33 2020) Epoch:  7  Training Cost:  628.26430719549 Training Accuracy:  0.962727271697738\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:16:34 2020) Epoch:  7  Test Cost:  628.3901000976563 Test Accuracy:  0.9179999947547912 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:17:30 2020) Epoch:  8  Training Cost:  584.7433194247159 Training Accuracy:  0.9713636392896826\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:17:31 2020) Epoch:  8  Test Cost:  584.8453491210937 Test Accuracy:  0.9349999904632569 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:18:27 2020) Epoch:  9  Training Cost:  544.0935460870916 Training Accuracy:  0.9745454557917335\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:18:28 2020) Epoch:  9  Test Cost:  544.1917114257812 Test Accuracy:  0.9349999904632569 \n",
            "\n",
            "learning rate:  5e-06\n",
            "(Thu Jun 25 20:19:24 2020) Epoch:  10  Training Cost:  506.1945814652876 Training Accuracy:  0.9722727320410989\n",
            "0\n",
            "50\n",
            "100\n",
            "150\n",
            "200\n",
            "(Thu Jun 25 20:19:25 2020) Epoch:  10  Test Cost:  506.2771057128906 Test Accuracy:  0.931000006198883 \n",
            "\n",
            "********************recall: [0.94387755 0.88636364]\n",
            "*****************precision: [0.97368421 0.78      ]\n",
            "******************f1_score: [0.95854922 0.82978723]\n",
            "(Thu Jun 25 20:19:26 2020) Final Test Cost:  506.2771057128906 Final Test Accuracy:  0.931000006198883\n",
            "**********(Thu Jun 25 20:19:26 2020) Train and Test NN End **********\n",
            "\n",
            "training examples: 2160\n",
            "test examples    : 240\n",
            "[0. 1.]\n",
            "learning rate:  5e-06\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-7ff64be2dc4b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    362\u001b[0m                   _ , c = session.run([optimizer, cost],\n\u001b[1;32m    363\u001b[0m                                     feed_dict={cnn_in: cnn_batch, rnn_in: rnn_batch, Y: batch_y, keep_prob: 1 - dropout_prob,\n\u001b[0;32m--> 364\u001b[0;31m                                                 phase_train: True})\n\u001b[0m\u001b[1;32m    365\u001b[0m                   \u001b[0mcost_history\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcost_history\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m                   \u001b[0mcount_cost\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    954\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    955\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 956\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    957\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1180\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1181\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1182\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1357\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1359\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1360\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1363\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1364\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1365\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1366\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1367\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m       return self._call_tf_sessionrun(options, feed_dict, fetch_list,\n\u001b[0;32m-> 1350\u001b[0;31m                                       target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1441\u001b[0m     return tf_session.TF_SessionRun_wrapper(self._session, options, feed_dict,\n\u001b[1;32m   1442\u001b[0m                                             \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1443\u001b[0;31m                                             run_metadata)\n\u001b[0m\u001b[1;32m   1444\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1445\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9cYcocDeYAl",
        "colab_type": "text"
      },
      "source": [
        "References: This code has been adopted from the github site supporting the paper \"\"Emotion Recognition from Multi-Channel EEG through Parallel Convolutional Recurrent Neural Network\":\n",
        "\n",
        "https://github.com/ynulonger/ijcnn\n"
      ]
    }
  ]
}